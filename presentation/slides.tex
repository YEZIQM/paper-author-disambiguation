\documentclass{beamer}
\usepackage{etex}

\usepackage[utf8]{inputenc}
\usepackage[frenchb]{babel}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{url}
\usepackage{moreverb}
\usepackage{fancyvrb}
\usepackage{minted}
\usepackage{natbib}
\usepackage{eulervm}
\usepackage{auto-pst-pdf}
\usepackage{pst-plot}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{tikz}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usetikzlibrary{shapes,arrows}
\tikzset{
  every overlay node/.style={
    draw=white,fill=white,anchor=north west,
  },
  every overlay node border/.style={
    draw=black,fill=white,anchor=north west,rounded corners,
  },
}
% Usage:
% \tikzoverlay at (-1cm,-5cm) {content};
% or
% \tikzoverlay[text width=5cm] at (-1cm,-5cm) {content};
\def\tikzoverlay{%
   \tikz[baseline,overlay]\node[every overlay node]
}%
\def\tikzoverlayborder{%
   \tikz[baseline,overlay]\node[every overlay node border]
}%

\newrgbcolor{mygreen}{.00 .5 .00}
\newcommand{\X}[1]{\textcolor{blue}{#1}}
\newcommand{\y}[1]{\textcolor{red}{#1}}
\newcommand{\model}[1]{\textcolor{mygreen}{#1}}
\newcommand{\loss}[1]{\textcolor{lightblue}{#1}}

\hypersetup{colorlinks=true, linkcolor=black, urlcolor=blue}
\usetheme{boxes}
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{sections/subsections in toc}[circle]
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{itemize subitem}[square]

\title{\bf Ethnicity Sensitive Author Disambiguation Using Semi-supervised Learning}
\author{Gilles Louppe, Hussein Al-Natsheh, Mateusz Susik \\and 
Eamonn Maguire}
\institute{International Conference on Knowledge Engineering and Semantic Web 2016, Prague}
\date{September 23, 2016}

\newcommand{\todo}[1]{\textcolor{red}{[TODO] #1}}

\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\begin{frame}
\titlepage
\end{frame}


% Motivation ==============================================================

\begin{frame}{Motivation}

Increasing number of publications means that every digital library needs a good
disambiguation solution. Without it:

\begin{enumerate}
    \item Users can spend a lot of time looking for a specific author if there are
    many authors with the same name in the database (homonym issues)
    \item Users are not able to find a specific author's publication if he uses
    multiple different names (synonym issues)
    \item It is not possible to present important date about authors such as citations
    or co-author network analysis.
\end{enumerate}

Our analysis was conducted on more than 1,2 millions of signatures from INSPIRE digital
library.

% IMPORTANT Explain what Inspire is.

\end{frame}

% Problem statement ==================================================================

% IMPORTANT Explain what the boxes represent.

\begin{frame}{Problem statement}
For each author, group together all his publications, and only those.

\footnotesize{
\begin{columns}[T]

\begin{column}{0.33\textwidth}
\begin{figure}
\texttt{M.S.Smith.1}
\includegraphics[width=\textwidth]{figures/no-more.png}
\end{figure}
\end{column}
\begin{column}{0.33\textwidth}
\begin{figure}
\texttt{Z.Liang.4}
\includegraphics[width=\textwidth]{figures/no-less.png}

\texttt{Z.Liang.5}
\includegraphics[width=\textwidth]{figures/no-less.png}

...

\texttt{Z.Liang.83}
\includegraphics[width=\textwidth]{figures/no-less.png}
\end{figure}
\end{column}
\begin{column}{0.33\textwidth}
\begin{figure}
\texttt{S.W.Hawking.1}
\includegraphics[width=\textwidth]{figures/exact.png}
\end{figure}
\end{column}

\end{columns}

\begin{columns}[T]
\begin{column}{0.33\textwidth}
\begin{center}{\it No more}\end{center}
\end{column}
\begin{column}{0.33\textwidth}
\begin{center}{\it No less}\end{center}
\end{column}
\begin{column}{0.33\textwidth}
\begin{center}{\it But all and only the correct ones}\end{center}
\end{column}
\end{columns}}
\end{frame}



% Definitions ====================================================================

\begin{frame} {Definitions}

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{./figures/fig-pub-to-signature.pdf}
\end{figure}

\end{frame}


% Author dismbiguation ====================================================================

\begin{frame}

\tikzstyle{block} = [rectangle, draw, text width=10em, text centered, rounded corners, minimum height=2em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = []

\begin{center}
\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (clustering) {Author disambiguation};
    \node [cloud, above of=clustering] (input1) {$\{s_1, s_2, ..., s_N\}$};
    \node [cloud, below of=clustering] (output) {$\{ \underbrace{\{ s_1, s_3, s_4 \}}_{\text{author 1}}, \underbrace{\{ s_2, s_5\}}_{\text{author 2}}, ... \}$};
    % Draw edges
    \path [line] (input1) -- (clustering);
    \path [line] (clustering) -- (output);
\end{tikzpicture}
\end{center}
\end{frame}



% Spread of the problem =======================================================

\begin{frame}{Spread of the problem}

As extracted from claimed publications in INSPIRE,

\begin{itemize}
\item {\color{blue} Authors have on average $2.06$ name variants}\\
Eg.: Doe, John; Doe, J.
\item {\color{blue} Unique name variants are shared on average by $1.04$ authors}
\end{itemize}

Clustering on exact full names or last name + first initial, should
yield very good results on average.

\vskip1em

{\color{red} But}, disambiguation issues are expected to amplify with the rise of Asian researchers:
Caucasian names (now representative of INSPIRE authors) are almost never
ambiguous, while Asian names are very often.

% compute average number of name variants per author (synonyms)
% compute average number of authors per name variants
% => easy but we want to do better

\end{frame}


% How would you fare? =========================================================

\begin{frame}{How would \textit{you} fare?}

\only<1-2>{
\begin{figure}
\includegraphics[scale=0.35]{figures/author1/1.png}\\[2em]
\includegraphics[scale=0.35]{figures/author1/2.png}\\[2em]
\uncover<2>{\color{blue} \cmark~ Same authors}
\end{figure}
}
\only<3-4>{
\begin{figure}
\includegraphics[scale=0.25]{figures/author2/3.png}\\[2em]
\includegraphics[scale=0.35]{figures/author2/1.png}\\[1em]
\uncover<4>{\color{blue} \cmark~ Same authors}
\end{figure}
}
% \only<5-6>{
% \begin{figure}
% \includegraphics[scale=0.35]{figures/author1/5.png}\\[2em]
% \includegraphics[scale=0.35]{figures/author1/4.png}\\[2em]
% \uncover<6>{\color{blue} \cmark~ Same authors}
% \end{figure}
% }
\only<5-6>{
\begin{figure}
\includegraphics[scale=0.35]{figures/author1/6.png}\\[2em]
\hspace*{-2cm}\includegraphics[scale=0.35]{figures/author2/2.png}\\[2em]
\uncover<6>{\color{red} \xmark~ Different authors}
\end{figure}
}
\only<7-8>{
\begin{figure}
\includegraphics[scale=0.35]{figures/author1/3.png}\\[2em]
\includegraphics[scale=0.35]{figures/author1/4.png}\\[2em]
\uncover<8>{\color{blue} \cmark~ Same authors}
\end{figure}
}

\end{frame}


% Learning ====================================================================

\begin{frame}{Learning from data}

\begin{itemize}
\item Manual disambiguation is {\color{red} long and difficult}, even for experienced curators.\\[2em]
\item Couldn't we {\color{blue} automatically find a set of rules} to disambiguate two signatures?

$$\varphi(s_1, s_2) = \begin{cases}
    0 & \text{if $s_1$ and $s_2$ belong to the same author},\\
    1 & \text{otherwise}.
  \end{cases}$$

\item This is a machine learning task called {\color{red}supervised learning}.
\end{itemize}

\end{frame}



% Pair-wise Features Extraction ========================================================

\begin{frame}{Pair-wise Features Extraction}

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{./figures/pairwise_features.png}
\end{figure}

\end{frame}


% Features ===================================================

\begin{frame} {Features}

\begin{itemize}
    \item {7 Ethnicity similarity features}
    \item {Co-authors similarity}
    \item {5 Name similarity features}
    \item {Metadata similarity measures:}
    \begin{itemize}
        \item {Affiliations}
        \item {Keywords}
        \item {Publication year difference}
        \item {Title}
        \item {Abstract}
        \item {Collaboration}
        \item {References}
        \item {Journal}
    \end{itemize}
\end{itemize}

\end{frame}

% Feature Selection by Recursive Elimination ========================================================

\begin{frame}{Feature Selection by Recursive Elimination}

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{./figures/fig-rfe.png}
\end{figure}

\end{frame}


% Pipeline ====================================================================

\begin{frame} {General Pipeline}


\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{./figures/fig-workflow.pdf}
\end{figure}

\begin{itemize}
\item Being able to disambiguate a pair of signatures does not end the problem. \\[0.5em]

\item Author disambiguation = {\color{red} clustering signatures that belong to the same author}.\\[0.5em]

\item Using our model (called also linkage function), the probability
      that two signatures belong to different authors can be used as a (pseudo) distance metric.
\end{itemize}

\end{frame}


% Hierarchical clustering =====================================================

\begin{frame}{Hierarchical Clustering}

\begin{columns}[T]

\begin{column}{0.33\textwidth}
\includegraphics[scale=0.75]{./figures/hierarchy.png}
\end{column}
\begin{column}{0.66\textwidth}
\begin{itemize}
\item General family of clustering algorithms that {\color{blue}build nested clusters by merging them successively}.\\[2em]

\item This hierarchy of clusters is represented as a tree (or dendrogram).\\[2em]

\item The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample.
\end{itemize}
\end{column}

\end{columns}

\end{frame}


% Implementation issues =====================================================

\begin{frame}{Issues}

\begin{itemize}
\item The complexity of hierarchical clustering is {\color{red} $O(N^2)$}. For $N=10^7$ signatures, this is impractical.

{\it Solution:} pre-cluster into blocks all signatures with the same last name + first initial, then cluster each of these blocks.\\[2em]

\item How do you set the {\color{red} cut-off threshold}?

{\it Solution:} using training data (e.g., claimed signatures), pick the threshold that locally maximizes some criterion.
\end{itemize}

\end{frame}

% Partitioning into Blocks =====================================================

% No reason to describe the dataset here

\begin{frame} {Solving Issue 1: Partitioning into Blocks}


\begin{columns}[T]

\begin{column}{0.5\textwidth}
\includegraphics[width=\textwidth]{./figures/blocking.png}
\end{column}
\begin{column}{0.5\textwidth}

With blocking, the complexity is practically linear. A good blocking algoriithm
should minimize:
\begin{itemize}


\item Number of pairs of signatures that belong to the same author but
after blocking step can not be clustered together. 
\item Sum of squares of the sizes of produced blocks.

\end{itemize}

\end{column}

\end{columns}

\end{frame}

% Cases =====================================================


\begin{frame} {Cases}

Analysis of data showed that author can have multiple names in few cases:

\begin {itemize}
 \item There are different
  ways of writing an author name, or signatures contain a typo
  (e.g., "Mueller, R." and "Muller, R.").

  \item An author has multiple surnames (or a patronymic) and some signatures place the first part of the surname within the given names (e.g., "Martinez Torres, A." and "Torres, A. Martinez").

  \item An author has multiple surnames and, on some signatures, only the first surname is
  present (e.g., "Smith-Jones, A." and "Smith, A.")

  \item An author has multiple given names and they are not always all recorded (e.g.,
  "Smith, Jack" and "Smith, A. J.")

  \item An authors surname changed (e.g., due to marriage).

\end {itemize}

\end{frame}

% Blocking Strategies =====================================================


\begin{frame} {Blocking Strategies}

Our solution:

\begin {itemize}
\item Group signatures with single family names by their phonetic represenations
(we tried Soundex, NYSIIS and Metaphone algorithms)
\item Match signatures with multiple given names with clusters that contain signatures
with the first given name from the new signatures.
\item Otherwise match signatures multiple given names with clusters that contain signatures
with the last given name from the new signatures.
\item Otherwise create new clusters.

\end {itemize}

This algorithm effectively solves paragraphs 1-3 from the previous page without introducing
many errors. As the blocks were still too large, we split them over the first given name
initital.

\end{frame}


% Cutting Strategy =====================================================


\begin{frame} {Solving Issue 2: Threshold Cut-off Strategy}

\includegraphics[width=\textwidth]{./figures/fig-cuts.pdf}


\end{frame}


% Evaluation =====================================================================

\begin{frame}{Evaluation}

{\it Protocol:} Use the {\color{blue} claimed signatures} (about 1M) to form {\color{blue} ground truth clusters}. Keep 10\% as a training set to find model parameters, and 90\% as a test set for evaluation.

\begin{align}
\text{$B^3$ Precision} &= \mathbb{E}_s \{ \frac{|\hat{C}(s) \cap C(s)|}{|\hat{C}(s)|}  \} \\
\text{$B^3$ Recall} &= \mathbb{E}_s \{ \frac{|\hat{C}(s) \cap C(s)|}{|C(s)|}  \} \\
\text{$B^3$ F-score} &= \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{align}

where $C(s)$ (resp., $\hat{C}(s)$) is the true (resp., predicted) set of signatures to which $s$ belongs.


% disambiguation is clustering
% how to evaluate clustering?
% give number for simple strategy (all same names baseline 0.5, all same block baseline 1.0)
\end{frame}

% Results =====================================================================

\begin{frame}{Results}

\begin{table}
    \centering
    \begin{tabular}{| l c |}
    \hline
        \textit{Method} & \textit{$B^3 \text{F-score}$}  \\
    \hline
    \hline
    Full name & 0.8183 \\
    Last name + First initial & 0.9409 \\
    \hline
    Our model & {\color{blue} 0.9868} \\
    \hline
    \end{tabular}
\end{table}

\end{frame}

% Results =====================================================================

\begin{frame}{Results 1 of 2}

\begin{table}
\caption{Average precision, recall and f-measure scores on test folds. CUnderlined row corresponds to the state-of-the-art choices.}
\centering
\begin{tabular}{|l|c c c |}
  \hline
                       & \multicolumn{3}{|c|}{\textbf{B3}}\\
  \textbf{Description} & $P$ & $R$ & $F$ \\
  \hline
  \hline
Baseline & 0.9024 & 0.9828 & 0.9409  \\
\hline
\underline{Blocking = SFI} & 0.9901 & 0.9760 & 0.9830  \\
Blocking = Double metaphone & 0.9856 & 0.9827 & 0.9841  \\
Blocking = NYSIIS & 0.9875 & 0.9826 & \textbf{0.9850}   \\
Blocking = Soundex & 0.9886 & 0.9745 & 0.9815  \\
\hline
\underline{Classifier = GBRT} & 0.9901 & 0.9760 & 0.9830   \\
Classifier = Random Forests & 0.9909 & 0.9783 & \textbf{0.9846}  \\
Classifier = Linear Regression & 0.9749 & 0.9584 & 0.9666 \\
\hline
Training pairs = Non-blocked, uniform & 0.9793 & 0.9630 & 0.9711   \\
Training pairs = Blocked, uniform & 0.9854 & 0.9720 & 0.9786   \\
\underline{Training pairs = Blocked, balanced} & 0.9901 & 0.9760 & \textbf{0.9830}  \\
\hline

\end{tabular}
\end{table}

\end{frame}

% Results =====================================================================

\begin{frame}{Results 2 of 2}

\begin{table}
\caption{Average precision, recall and f-measure scores on test folds. Underlined row corresponds to the state-of-the-art choices.}
\centering
\begin{tabular}{|l|c c c |}
  \hline
                       & \multicolumn{3}{|c|}{\textbf{B3}}\\
  \textbf{Description} & $P$ & $R$ & $F$ \\
  \hline
  \hline
Baseline & 0.9024 & 0.9828 & 0.9409  \\
\hline
\underline{Clustering = Average linkage} & 0.9901 & 0.9760 & \textbf{0.9830}  \\
Clustering = Single linkage & 0.9741 & 0.9603 & 0.9671  \\
Clustering = Complete linkage & 0.9862 & 0.9709 & 0.9785   \\
\hline
No cut (baseline) & 0.9024 & 0.9828 & 0.9409   \\
Global cut & 0.9892 & 0.9737 & 0.9814   \\
\underline{Block cut} & 0.9901 & 0.9760 & \textbf{0.9830} \\
\hline
\textbf{Combined best settings} & 0.9888 & 0.9848 & \textbf{0.9868}  \\
Best settings without ethnicity features & 0.9862 & 0.9819 & 0.9841 \\
\hline

\end{tabular}
\end{table}

\end{frame}

% Results =====================================================================

\begin{frame}{Summary Results}

\begin{table}
    \centering
    \begin{tabular}{| l c |}
    \hline
        \textit{Method} & \textit{$B^3 \text{F-score}$}  \\
    \hline
    \hline
    Full name & 0.8183 \\
    Last name + First initial & 0.9409 \\
    \hline
    Our model & {\color{blue} 0.9868} \\
    \hline
    \end{tabular}
\end{table}

\end{frame}


% Performance ================================================================

\begin{frame}{Integration}

The solution is currently being used by the INSPIRE and INVENIO projects at CERN. \\[1em]

Execution time: $20$ hours to process the complete set of the labeled data (for 10M
signatures, on a $16$ cores machine with $32$GB of RAM). It it possible to do
a much faster incremental disambiguation by running the clustering step only over the blocks
with new signatures.

\end{frame}

% Future Work ================================================================

\begin{frame}{Future Work}

\begin{itemize}

\item Error analysis.\\[1em]

% I've read the paper and it does not seem like a very promising idea to me. - Mateusz
\item Explore Author2Vec approach as a partitioning (blocking) strategy.\\[1em]

\item Build more comprehensive name-ethnicity dataset. \\[1em]

\item Build phonetic algorithm tailored to the disambiguation task. \\[1em]

\item Archive and utilize user's feedback to enhance the model. \\[1em]

\item Try our disambiguation solution for other types of entities. 

\end{itemize}

\end{frame}


\end{document}
